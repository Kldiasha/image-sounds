{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "from numba import core\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.0.post2\n"
     ]
    }
   ],
   "source": [
    "def padd_with_zeros(twod_numpy, w, h):\n",
    "    old_w = twod_numpy.shape[0]\n",
    "    old_h = twod_numpy.shape[1]\n",
    "    h = max(old_h, h)\n",
    "    w = max(old_w, w)\n",
    "    lx = (w - old_w) // 2\n",
    "    rx = w - lx - old_w\n",
    "    ly = (h - old_h) // 2\n",
    "    ry = h - ly - old_h\n",
    "    return np.pad(twod_numpy, pad_width=((lx, rx), (ly, ry)), mode='constant')\n",
    "def process_input_file(filename):\n",
    "\n",
    "    waveform, sample_rate = librosa.load(filename)\n",
    "\n",
    "    n_fft = 512  # Number of FFT points (window size)\n",
    "    hop_length = 1024  # Hop length (frame shift)\n",
    "\n",
    "    # Compute the STFT\n",
    "    stft = librosa.stft(waveform, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    padded = padd_with_zeros(stft, 260, 90)\n",
    "    return  padded\n",
    "\n",
    "\n",
    "# filename='./data/LydoK7hXKbs-audio/audio0-10-23.00.wav'\n",
    "filename='./data/a_oqcg0hvpo-audio/audio0-55-27.00.wav'\n",
    "#define the beginning time of the signal\n",
    "\n",
    "print(librosa.__version__)\n",
    "\n",
    "data_point = process_input_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_oqcg0hvpo\n",
      "Gz99TTxmvls\n",
      "ubz5lz_l7IY\n",
      "LydoK7hXKbs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "video_files=[\n",
    "    'a_oqcg0hvpo',\n",
    "    'Gz99TTxmvls',\n",
    "    'ubz5lz_l7IY',\n",
    "    'LydoK7hXKbs',\n",
    "]\n",
    "\n",
    "tensors = {}\n",
    "\n",
    "def get_prediction(video_name, time_stamp):\n",
    "    global tensors\n",
    "    local_name = f'{video_name}{time_stamp[:3]}'\n",
    "    if local_name not in tensors:\n",
    "        t = torch.load(f'./resnet_predictions/{video_name}/{local_name}.pth')\n",
    "        tensors[local_name] = t\n",
    "    else:\n",
    "        t = tensors[local_name]\n",
    "    return t[f'frame{time_stamp}']\n",
    "\n",
    "\n",
    "DATA_PER_FILE = 50\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for video_name in video_files:\n",
    "    subdir = os.listdir('./data/' + video_name + '-audio')\n",
    "    files = random.choices(subdir, k=DATA_PER_FILE)\n",
    "    for file in files:\n",
    "        time_stamp = file[5:file.find('.')]\n",
    "        label = video_name + '-' + time_stamp\n",
    "        audio_data = process_input_file(f'./data/{video_name}-audio/{file}')\n",
    "        classification_res = get_prediction(video_name, time_stamp)\n",
    "\n",
    "        X.append(np.abs(audio_data))\n",
    "        Y.append(classification_res)\n",
    "    print(video_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, shuffle=True)\n",
    "BATCH_SIZE = 40\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.io.wavfile as wavfile\n",
    "from scipy import signal\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential( # 260 x 90\n",
    "                nn.Conv2d(1, 16, stride=(1, 1), kernel_size=(3, 3), padding='same'),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                # nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1), return_indices=True),\n",
    "                nn.Conv2d(16, 32, stride=(1, 1), kernel_size=(3, 3), padding='same'),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Dropout(0.2),\n",
    "                # nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1), return_indices=True),\n",
    "                # nn.Conv2d(32, 32, stride=(1, 1), kernel_size=(3, 3), padding='same'),\n",
    "                # nn.LeakyReLU(0.01),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(748800, 1000)\n",
    "        )\n",
    "        self.classification = nn.Sequential(\n",
    "                nn.Softmax(1)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(1000, 748800),\n",
    "                nn.Unflatten(1, (64, 260, 90)),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                # nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                # nn.MaxUnpool2d(kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "                nn.ConvTranspose2d(32, 16, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                # nn.MaxUnpool2d(kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.ConvTranspose2d(16, 1, stride=(1, 1), kernel_size=(3, 3), padding=1),\n",
    "                nn.LeakyReLU(0.01),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        classified = self.classification(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, classified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [09:39<6:16:53, 579.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [25:57<8:35:18, 813.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [46:02<10:11:58, 992.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [59:27<9:11:08, 918.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [1:09:34<7:50:20, 806.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [1:19:33<6:56:54, 735.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [1:29:37<6:20:57, 692.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [1:39:09<5:48:55, 654.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [1:48:32<5:23:19, 625.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [1:58:43<5:10:37, 621.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [2:08:42<4:56:57, 614.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [2:19:01<4:47:18, 615.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [2:28:46<4:32:53, 606.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [2:40:14<4:33:30, 631.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [2:49:41<4:14:51, 611.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [3:00:06<4:06:14, 615.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [3:10:56<4:00:00, 626.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [3:21:49<3:52:33, 634.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [3:32:05<3:40:02, 628.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [3:42:41<3:30:19, 631.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [3:52:50<3:17:43, 624.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [4:03:05<3:06:25, 621.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [4:13:18<2:55:20, 618.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [4:24:16<2:48:09, 630.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [4:34:12<2:35:05, 620.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [4:43:47<2:21:34, 606.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [4:53:58<2:11:43, 607.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [5:04:17<2:02:16, 611.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [5:14:12<1:51:11, 606.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [5:24:12<1:40:44, 604.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [5:34:35<1:31:28, 609.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [5:46:13<1:24:51, 636.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [5:58:24<1:17:33, 664.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [6:10:18<1:07:56, 679.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [6:22:22<57:45, 693.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [6:33:55<46:11, 692.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [6:43:57<33:16, 665.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [6:55:19<22:21, 670.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [7:06:23<11:08, 668.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [7:16:48<00:00, 655.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Average Loss: 7.882465565204621\n",
      "Elapsed 0 number of epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 260, 90])\n",
      "torch.Size([40, 1, 260, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [04:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m X_pred, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m constr_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss_fn(X_pred, X_batch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     45\u001b[0m classified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification(encoded)\n\u001b[0;32m---> 46\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded, classified\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "autoencoder = AutoEncoder((260, 90))\n",
    "\n",
    "# Define the loss function\n",
    "reconstruction_loss_fn = nn.MSELoss()\n",
    "classication_loss_fn = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder.to(device)\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"hello\")\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Iterate over the training dataset\n",
    "    for X_batch, y_batch in tqdm(train_loader):\n",
    "        print(X_batch.shape)\n",
    "        X_batch = X_batch.view(BATCH_SIZE, -1, 260, 90).to(device)\n",
    "        print(X_batch.shape)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        X_pred, y_pred = autoencoder(X_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        constr_loss = reconstruction_loss_fn(X_pred, X_batch)\n",
    "        classi_loss = classication_loss_fn(y_pred, y_batch)\n",
    "        loss = constr_loss + classi_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss}\")\n",
    "\n",
    "    end = time.time()\n",
    "    dur = end - start\n",
    "    if dur > 3600:\n",
    "        print(f\"Elapsed {epoch} number of epochs.\")\n",
    "        num_epochs = epoch\n",
    "        break\n",
    "\n",
    "weight_file_name = f'model_{num_epochs}_{len(train_loader)}'\n",
    "torch.save(autoencoder.state_dict(), weight_file_name)\n",
    "\n",
    "# # Example usage after training\n",
    "# test_dataset = MNIST(root='data', train=False, transform=ToTensor(), download=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         images, _ = batch\n",
    "#         images = images.view(-1, 784).to(device)\n",
    "#         reconstructed = autoencoder(images)\n",
    "\n",
    "#         # Perform any further processing or visualization with the reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file_name = f'model_{num_epochs}_{len(train_loader)}'\n",
    "torch.save(autoencoder.state_dict(), weight_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as transforms\n",
    "import torchaudio\n",
    "import scipy\n",
    "with torch.no_grad():\n",
    "    # Pass the test data through the model\n",
    "    test_data = train_dataset[0]\n",
    "    test_data = test_data[np.newaxis, ...]\n",
    "    test_data = torch.from_numpy(test_data)\n",
    "    reconstructed_stft, classified = autoencoder.forward(test_data)\n",
    "    \n",
    "    test_output_stft = reconstructed_stft[:, 1: -2, 1: -2]\n",
    "\n",
    "    n_fft = 512  # Number of FFT points (window size)\n",
    "    hop_length = 1024  # Hop length (frame shift)\n",
    "\n",
    "    # Create the STFT and iSTFT transforms\n",
    "    transform_stft = transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
    "    transform_istft = transforms.GriffinLim(n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # Reconstruct audio from the output STFT batch\n",
    "    # test_reconstructed_waveform= transform_istft(test_output_stft)\n",
    "    t, x = scipy.signal.istft(test_output_stft)\n",
    "    a, b = scipy.io.wavfile.read(audio_filenames[0])\n",
    "    print(b.shape)\n",
    "    x = x.transpose()\n",
    "    print(x.shape)\n",
    "    # Save the test examples\n",
    "    scipy.io.wavfile.write('tmp.wav', 6000, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
